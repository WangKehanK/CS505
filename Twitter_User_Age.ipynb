{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter User Age.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WangKehanK/CS505/blob/main/Twitter_User_Age.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnhsDxyyH_T6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import json\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b64JjLDICc2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNRGN9igIHA9"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Twitter_user_handles_to_predict.csv\", encoding = \"utf-8\")\n",
        "df_labeled = pd.read_csv(\"/content/drive/My Drive/Twitter_users_labeled_with_age_and_gender.csv\", encoding = \"latin-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMiwFK6uHbEf"
      },
      "source": [
        "# Get authentication for tweepy from auth.txt\n",
        "# auth.txt was uploaded before in order to use m3inference\n",
        "keys = ''\n",
        "with open(f'/content/drive/MyDrive/auth.txt', 'r') as reader:\n",
        "  keys=reader.read()\n",
        "keys = keys.split('\\n')\n",
        "# for i in range(len(keys)):\n",
        "#   keys[i] = keys[i].split('\"')[1];\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxEuKZ5-HvF3"
      },
      "source": [
        "# Setup tweepy\n",
        "import tweepy\n",
        "api_key = keys[0]\n",
        "api_secret = keys[1]\n",
        "access_token = keys[2]\n",
        "access_secret = keys[3]\n",
        "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
        "auth.set_access_token(access_token, access_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFVNbTG1HxEG"
      },
      "source": [
        "# Try first user\n",
        "user = df['Username']\n",
        "userID = user[0]\n",
        "print(userID, len(user))\n",
        "tweets = api.user_timeline(screen_name=userID,count=100,include_rts = False,tweet_mode = 'extended')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46QbvbMMHzCV"
      },
      "source": [
        "# Extract full text from tweets\n",
        "# for testing purpose, print user's 3 most recent tweets\n",
        "\n",
        "tweet_text = [tweet._json['full_text'] for tweet in tweets]\n",
        "for info in tweets[:3]:\n",
        "    print(\"ID: {}\".format(info.id))\n",
        "    print(info.created_at)\n",
        "    print(info.full_text)\n",
        "    print(\"\\n\")\n",
        "print(len(tweet_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XkSsXKIjPeJ"
      },
      "source": [
        "# Extract latest 100 tweets using api.user_timeline for each user\n",
        "import time\n",
        "\n",
        "all_tweets = []\n",
        "for sleep_count, userID in enumerate(user):\n",
        "  try:\n",
        "    tweets = api.user_timeline(screen_name=userID, \n",
        "                              count=100,\n",
        "                              include_rts = False,\n",
        "                              tweet_mode = 'extended')\n",
        "    all_tweets.extend(tweets)\n",
        "    print('Number of tweets extracted {}'.format(len(all_tweets)))\n",
        "\n",
        "    if (sleep_count %450 == 0 and not sleep_count == 0):\n",
        "      time.sleep(15*60)\n",
        "      print(\"Taking a sleep break...\" + '\\n')\n",
        "      \n",
        "  except tweepy.TweepError as e:\n",
        "    # print(e.response.status_code, e.reason)\n",
        "    # the user not found error\n",
        "    print(\"screen_name that failed=\",  userID)\n",
        "    pass\n",
        "\n",
        "print(\"Extraction Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgG8EVOvkjon"
      },
      "source": [
        "outtweets = [[tweet.id_str, \n",
        "              tweet.created_at, \n",
        "              tweet.favorite_count, \n",
        "              tweet.retweet_count, \n",
        "              tweet.full_text.encode(\"utf-8\").decode(\"utf-8\")] \n",
        "             for idx,tweet in enumerate(all_tweets)]\n",
        "\n",
        "df = DataFrame(outtweets,columns=[\"id\",\"created_at\",\"favorite_count\",\"retweet_count\", \"text\"])\n",
        "df.to_csv('%all_tweets.csv' % userID,index=False)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0cZRm2DH3eu"
      },
      "source": [
        "\n",
        "TODO: Using BERT to extract fixed feature vectors\n",
        "\n",
        "https://github.com/google-research/bert#using-bert-to-extract-fixed-feature-vectors-like-elmo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdHAe_1SH3Ec"
      },
      "source": [
        "age_no_na = df_labeled[df_labeled['human.labeled.age'].notna()]\n",
        "age_no_na"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}